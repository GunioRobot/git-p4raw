#!/usr/bin/perl

use strict vars, subs;
use warnings;
use Scriptalicious;
use FindBin qw($Bin);
use List::Util qw(sum);
use Data::Dumper;
use DBI;

sub do_init {
	my $dbh = shift;
	$dbh->begin_work;
	say "Setting up tables on DB ".$dbh->{pg_db}." (in a transaction)";
	open SQL, "<$Bin/tables.sql" or die $!;
	my $statements = join "", <SQL>;
	close SQL;
	mutter "Running: $statements";
	eval {
		local($dbh->{PrintError});
		$dbh->do($statements);
		$dbh->commit;
	};
	if ( $@ ) {
		my $msg = $@;
		chomp($msg);
		$dbh->rollback;
		barf "couldn't set up tables - already setup? ($msg)";
	}
	say "Set up DB OK";
}

sub do_drop {
	my $dbh = shift;
	open SQL, "<$Bin/tables.sql" or die $!;
	my $statements = join ";\n",
		map { m{create table (\S+)} ? ("drop table $1") : () }
			<SQL>;
	close SQL;
	mutter "Running: $statements";
	$dbh->do($statements);
	say "Dropped DB OK";
}

sub do_load {
	my $dbh = shift;
	while ( my $filename = shift ) {
		if ( $filename =~ m{\.gz$} ) {
			$filename = "zcat $filename|";
		}
		my %wanted =
			( "db.desc" => "change_desc",
			  "db.integed" => "integed",
			  "db.change" => "change",
			  "db.revcx" => "revcx",
			  "db.user" => "p4user",
			  "db.rev" => "rev",
			  "db.label" => "label",
			);
		my %sth;
		my $get_sth = sub {
			my ($table, $size)=@_;
			$sth{$table."\0".$size} ||= do {
				$dbh->commit;
				$dbh->begin_work;
				my $sql = "INSERT INTO $table "
					."VALUES (".join(",",("?")x$size).")";
				whisper "Preparing: $sql";
				$dbh->prepare($sql);
			};
		};
		my $wanted_re = join "|", keys %wanted;
		my $regex_wanted = qr/(?:$wanted_re)/;
		open JOURNAL, "$filename" or die $!;
		#binmode JOURNAL, ":encoding(iso-8859-1)";
		my ($rows, $dirty, $dirty_rows);
		$dbh->begin_work;
		$dbh->{AutoCommit} = 0;
		$dbh->{PrintError} = 0;
		my %count;
		while ( <JOURNAL> ) {
			next unless m{^\@pv\@\s\d+\s@($regex_wanted)@};
			my $db = $wanted{$1};
			my @columns;
			while ( (pos($_)||0)+1 < length $_ ) {
				my $pre = pos $_;
				my $ok = m{\G(?:\@((?:[^@]+|\@\@)*)\@|(-?\w+))\s}g;
				if ( !$ok ) {
					pos($_) = $pre;
				}
				my ($string, $token) = ($1, $2) if $ok;
				if ( defined $string ) {
					$string =~ s{\@\@}{\@}g;
					utf8::upgrade($string);
					push @columns, $string;
				}
				elsif ( defined $token ) {
					push @columns, $token;
				}
				else {
					die "end of file; $_" if eof JOURNAL;
					my $p = pos $_;
					pos($_)=0;
					my $line;
					my @extra;
					do {
						$line = readline JOURNAL;
						push @extra, $line;
					} until ($line =~ m{@\s*$});
					$_.=join("",@extra);
					pos($_) = $p;
					redo;
				}
			}
			@columns = (@columns[3..$#columns]);
			my $sth = $get_sth->($db,scalar(@columns));
			eval {
				$sth->execute(@columns);
				$rows++;
				$dirty_rows++;
				$dirty+=sum map { length } @columns;
				if ( $VERBOSE > 1 ) {
					print "Saved: $columns[0]\n";
				}
			};
			if ( $@ ) {
				barf "DBI error ($@); Data: ".Dumper(\@columns);
			}
			$count{$db}++;
			if ( $dirty > (1<<18) or $dirty_rows >= 5000 ) {
				say "commit after $rows rows: ".
					join("; ", map{"$count{$_} x $_"}
					     keys %count);
				%count=();
				$dirty = 0;
				$dirty_rows = 0;
				$dbh->commit;
				$dbh->begin_work;
			}
		}
		$dbh->commit;
		$dbh->{AutoCommit} = 1;
	}
}

sub do_blobs {
	my $dbh = shift;
	# psuedocode for blob import.
	# 1. find the lowest change which is yet to be marked as
	#    imported
	# 2. get all of the files in it
	# 3. for each of those files, find all the ones without
	#    rev_blob rows
	# 4. for each of those files, find *all* of the branched
	#    versions of it, and send them all at once to
	#    git-fast-import, entering rev_blob rows for them as we
	#    get hashes back from git-fast-import
	#    It's quite important to do all the revisions of a file at
	#    once, otherwise fast-import will not be able to make
	#    on-the-fly deltas and the repo will become gigabytes.
	# 
	#    note "p4 print" is not required; can just use rcs
	#    directly by looking at the "rev" table; it has a rcs
	#    filename and revision that quite adequately refers to an
	#    rcs version.  So you can just collect
	#    `co -p1.2 -kb depot/mg.c` (eg), get its length, confirm
	#    the MD5, and then feed to git-fast-import using the
	#    "mark" functionality, perhaps marking it with the MD5
	#    or depotpath/revision.  Then when the fast-import
	#    "checkpoint" command is issued we will get back the
	#    GIT-SHA1 values.
	#    In fact I'd be highly tempted to parse the RCS file
	#    directly, as it might be significantly faster to hold
	#    the latest version of each RCS file in memory, as we work
	#    backwards through the revisions and construct new
	#    versions of it for feeding to fast-import.  Consider an
	#    RCS file with many revisions; getting all revisions out
	#    with `co -p` will be O(N^2) but directly will be O(N)

	# 5. finally, we have rev_blob rows for all of the files
	#    in this version, so send a tree and/or commit object
	#    (though it might be easier to use
	#    `git update-index --index-info`).  Information from the
	#    integed table should probably go into the commit message,
	#    where it is not redundant (which is a difficult criterion
	#    to nail down for sure!)
}

#=====================================================================
#  MAIN SECTION STARTS HERE
#=====================================================================
getopt();

binmode STDOUT, ":utf8";

my $action = shift;
abort "no action given" unless $action;
abort "bad action/argument `$action'" unless defined &{"do_$action"};

my $dbh = DBI->connect("dbi:Pg:");
$dbh->{RaiseError}=1;
$dbh->{PrintError}=1;
$dbh->{AutoCommit}=1;

&{"do_$action"}($dbh, @ARGV);

