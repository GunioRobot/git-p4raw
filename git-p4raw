#!/usr/bin/perl

use strict vars, subs;
use warnings;
use Scriptalicious;
use FindBin qw($Bin);
use List::Util qw(sum max min);
use Scalar::Util qw(looks_like_number);
use Data::Dumper;
use DBI;
use Cwd;
use IO::Handle;
use File::Path qw(rmtree);
use Fatal qw(:void open);
use Digest::SHA1;
use Digest::MD5 qw(md5_hex);
BEGIN { eval "use Term::ProgressBar" };

use vars qw($SHARE $SCRIPT_MODE);
$SHARE = $Bin;

sub do_init {
	my $dbh = shift;
	run("git", "init");
	run("git", "config", "p4raw.source", getcwd);
	$dbh->begin_work;
	say "Setting up tables on DB ".$dbh->{pg_db}." (in a transaction)";
	open SQL, "<$SHARE/tables.sql" or die $!;
	my $statements = join "", <SQL>;
	close SQL;
	mutter "Running: $statements";
	eval {
		local($dbh->{PrintError});
		$dbh->do($statements);
		$dbh->commit;
	};
	if ( $@ ) {
		my $msg = $@;
		chomp($msg);
		$dbh->rollback;
		barf "couldn't set up tables - already setup? ($msg)";
	}
	say "Set up DB OK";
}

sub do_drop {
	my $dbh = shift;
	open SQL, "<$SHARE/tables.sql" or die $!;
	my $statements = join ";\n", reverse
		map { (m{create (table|sequence) (\S+)}
		       ? ("drop $1 $2"
			  . ($1 eq "table" ? " cascade" : ""))
		       : ()) } <SQL>;
	close SQL;
	mutter "Running: $statements";
	$dbh->begin_work;
	$dbh->do("set constraints all deferred");
	$dbh->do($statements);
	$dbh->commit;
	say "Dropped DB OK";
	my $git_dir = capture("git", "rev-parse", "--git-dir");
	my $source = capture("git", "config", "p4raw.source");
	if ( $source ) {
		if ( $source eq getcwd ) {
			rmtree($git_dir);
		}
		else {
			moan("source is '$source', not ".getcwd
			     ."; rm -rf $git_dir yourself");
		}
	}
	else {
		moan "no p4raw.source; not removing no git dir";
	}
}

sub do_load {
	my $dbh = shift;
	my @files;
	if ( @_ ) {
		@files = @_;
	}
	else {
		@files = grep { -f }
			(<journal checkpoint journal.*.gz checkpoint.*.gz>,
			 <p4raw-extra-*.asv>);
	}
	while ( my $filename = shift @files ) {
		say "loading data from $filename";
		start_timer;
		if ( $filename =~ m{\.gz$} ) {
			$filename = "zcat $filename|";
		}
		my %wanted =
			( "db.desc" => "change_desc",
			  "db.integed" => "integed",
			  "db.change" => "change",
			  "db.revcx" => "revcx",
			  "db.user" => "p4user",
			  "db.rev" => "rev",
			  "db.label" => "label",
			);
		my %sth;
		my $get_sth = sub {
			my ($table, $size)=@_;
			$sth{$table."\0".$size} ||= do {
				my $sql = "INSERT INTO $table "
					."VALUES (".join(",",("?")x$size).")";
				whisper "Preparing: $sql";
				$dbh->prepare($sql);
			};
		};
		my $wanted_re = join "|", keys %wanted;
		my $regex_wanted = qr/(?:$wanted_re)/;
		open JOURNAL, "$filename" or die $!;
		#binmode JOURNAL, ":encoding(iso-8859-1)";
		my ($rows, $dirty, $dirty_rows);
		$dbh->begin_work;
		$dbh->{AutoCommit} = 0;
		$dbh->{PrintError} = 0;
		my %count;
		while ( <JOURNAL> ) {
			next unless m{^\@pv\@\s\d+\s@($regex_wanted)@};
			my $db = $wanted{$1};
			my @columns;
			while ( (pos($_)||0)+1 < length $_ ) {
				my $pre = pos $_;
				my $ok = m{\G(?:\@((?:[^@]+|\@\@)*)\@|(-?\w+))\s}g;
				if ( !$ok ) {
					pos($_) = $pre;
				}
				my ($string, $token) = ($1, $2) if $ok;
				if ( defined $string ) {
					$string =~ s{\@\@}{\@}g;
					utf8::upgrade($string);
					push @columns, $string;
				}
				elsif ( defined $token ) {
					push @columns, $token;
				}
				else {
					die "end of file; $_" if eof JOURNAL;
					my $p = pos $_;
					pos($_)=0;
					my $line;
					my @extra;
					do {
						$line = readline JOURNAL;
						push @extra, $line;
					} until ($line =~ m{@\s*$});
					$_.=join("",@extra);
					pos($_) = $p;
					redo;
				}
			}
			@columns = (@columns[3..$#columns]);
			my $sth = $get_sth->($db,scalar(@columns));
			eval {
				$sth->execute(@columns);
				$rows++;
				$dirty_rows++;
				$dirty+=sum map { length } @columns;
				if ( $VERBOSE > 1 ) {
					print "Saved: $columns[0]\n";
				}
			};
			if ( $@ ) {
				barf "DBI error ($@); Data: ".Dumper(\@columns);
			}
			$count{$db}++;
			if ( $dirty > (1<<18) or $dirty_rows >= 5000 ) {
				say "commit after $rows rows: ".
					join("; ", map{"$count{$_} x $_"}
					     keys %count);
				%count=();
				$dirty = 0;
				$dirty_rows = 0;
				$dbh->commit;
				$dbh->begin_work;
			}
		}
		$dbh->commit;
		$dbh->{AutoCommit} = 1;
		say "Loaded $rows rows from $filename in ".show_elapsed;
	}
}

sub _to_p4_journal_format {
	my $db_name = shift;
	return
		((join " ", map {
			my $x = $_;
			if ( looks_like_number $x ) {
				$x
			}
			else {
				$x =~ s{\@}{\@\@}g;
				"\@$x\@"
			}
		} ("pv", 3, "db.$db_name", @_))."\n");
}

sub _journal_p4_row {
	my $statefile = shift;
	my $table = shift;
	my $mode = ( -e $statefile ? ">>" : ">" );
	open JOURNAL, $mode, $statefile;
	print JOURNAL _to_p4_journal_format $table, @_;
	close JOURNAL;
}

sub add_p4users {
	my $dbh = shift;
	my $data = shift;
	my $statefile = "p4raw-extra-users.asv";
	for my $datum ( @$data ) {
		my ($who, $count) = @$datum;
		say "$who made $count changes";
		my ($realname, $email);
		do {
			$realname = prompt_string
				"Who was using this '$who' moniker?", $realname;
			say "'$realname' huh.  Ok.";
			$email = prompt_string
				("And what was (or is) their e-mail address?",
				 $email);
			say "Right, so I'll attribute commits from that usercode "
				."to $realname <$email>";
		} until ( prompt_Yn("Sound good?") );
		_journal_p4_row $statefile, "user",
			($who, $email, "", time, time,
			 $realname, "", 0, "", 0);
	}
	return $statefile;
}

sub do_check {
	my $dbh = shift;

	if ( $VERBOSE > 0 ) {
		require Text::CSV_XS;
	}

	open SQL, "<$SHARE/constraints.sql" or die $!;
	my $constraints = join "", <SQL>;
	close SQL;
	my $do_this;
	my $one_row;

	while ( $constraints =~ m{\G(?: ( \s* --(?-s:.*)
					  (?: \n\s*--(?-s:.*) )* )
			            | \s* (.*?) (?:;|\Z) ) }sgx ) {
		my ($comment, $sql) = ($1, $2);
		if ( $comment ) {
			$comment =~ s{^\s*--\s*}{}mg;
			if ( $comment =~ s{^FOUND: ((?-s:.*))\n}{}ms ) {
				$do_this = $1;
			}
			else {
				undef($do_this);
			}
			if ( $comment =~ s{^ONEROW(?-s:.*)\n}{}ms ) {
				$one_row = 1;
			}
			else {
				undef($one_row);
			}
			say $comment;
		}
		elsif ( $sql ) {
			$sql =~ s{^\s*}{}s;
			if ( $sql =~ m{^select}i ) {
				mutter "query: $sql";
				my $sth = $dbh->prepare($sql);
				$sth->execute;
				my $csv;
				if ( $VERBOSE > 0 or $one_row ) {
					$csv = Text::CSV_XS->new
						({binary => 1,
						  eol => "\n"});
					my @N = @{ $sth->{NAME} };
					$csv->print(\*STDOUT, \@N);
				}
				my $rows = 0;
				my @data;
				while ( my @row = $sth->fetchrow_array ) {
					if ( $csv ) {
						$csv->print(\*STDOUT, \@row)
							or barf $csv->error;
					}
					if ( $do_this ) {
						push @data, \@row;
					}
					$rows++;
				}
				if ( !$one_row ) {
					say "($rows rows".
						($rows&&$VERBOSE==0
						 ?"; use -v to see them"
						 :"").")";
				}
				if ( $do_this and $rows ) {
					no strict 'refs';
					my $statefile =
						&{"$do_this"}($dbh, \@data);
					if ( $statefile ) {
						eval {
							do_load($dbh, $statefile);
						};
						moan("load of new data failed; $@")
							if $@;
					}
				}
			}
			else {
				mutter "running: $sql";
				eval { local($dbh->{PrintError});
				       $dbh->do($sql) };
				if ( $@ ) {
					my $x = $@;
					chomp($x);
					say "error from DB ($x), continuing"
						unless $x =~ m{already exists};
				}
			}
		}
	}
}

sub do_find_change {
	my $dbh = shift;
	my $rev = shift;
	$rev or abort "no revision passed to find-change";

	if ( $rev =~ m{^\d{1,6}$} ) {
		show_git_paths($dbh, $rev);
	}
	elsif ( $rev =~ m{^[a-f0-9]{40}$} ) {
		show_p4_change($dbh, $rev);
	}
	else {
		my ($rc, $revision) = capture_err
			(-out2 => "/dev/null",
			 qw(git rev-parse --verify), $rev);

		if ( $? ) {
			barf "'$rev' is not a valid revision";
		}
		chomp($revision);
		show_p4_change($dbh, $revision);
	}
}

sub show_p4_change {
	my $dbh = shift;
	my $git_rev = shift;

	my $query = $dbh->prepare(<<SQL);
select
	cm.change,
	cm.branchpath
from
	change_marks cm
	inner join marks m using (mark)
where
	m.commitid = ?
SQL
	$query->execute($git_rev);
	my $x = $query->fetchrow_hashref;
	$query->finish;

	if ( !$SCRIPT_MODE ) {
		if ( $x ) {
			say "commit ".substr($git_rev, 0, 12)
				.(" is Change $x->{change} on branch "
				  .$x->{branchpath});
		}
		else {
			barf "commit $git_rev not found in DB";
		}
	}
	else {
		if ( $x ) {
			print "$x->{change},$x->{branchpath}\n";
		}
		else {
			exit 1;
		}
	}
}

sub show_git_paths {
	my $dbh = shift;
	my $change = shift;

	my $query = $dbh->prepare(<<SQL);
select
	m.commitid,
	cm.branchpath
from
	change_marks cm
	inner join marks m using (mark)
where
	cm.change = ?
SQL
	$query->execute($change);
	my @d;
	while ( my $x = $query->fetchrow_hashref ) {
		push @d, $x;
	}
	$query->finish;

	if ( !$SCRIPT_MODE) {
		if ( @d > 1 ) {
			say "change $change affected multiple branches:";
			for ( @d ) {
				print "branch $_->{branchpath}, see "
					."commit $_->{commitid}\n";
			}
		}
		elsif ( @d ) {
			say "change $change was on branch "
				.("$d[0]{branchpath}, see commit "
				  .$d[0]{commitid}."\n");
		}
		else {
			barf "change $change not found in DB.  perhaps it was cancelled?";
		}
	}
	else {
		if ( @d ) {
			print "$_->{commitid},$_->{branchpath}\n"
				for @d;
		}
		else {
			exit 1;
		}
	}
}


sub do_filelog {
	my $dbh = shift;
	my %filelog_opts;
	{
		@ARGV = @_;
		getopt("i" => \$filelog_opts{follow_branches},
		       "t" => \$filelog_opts{show_time},
		       "l" => \$filelog_opts{show_desc},
		       "L" => \$filelog_opts{show_some_desc},
		       "m=i" => \$filelog_opts{maxRevs},
		      );
		@_ = @ARGV;
	}

	my ($follow_branches, $show_type, $show_desc, $show_some_desc,
	    $maxRevs);
	my @placeholders;
	my $pathspec = shift;
	my ($depotpath, $rev) = ($pathspec =~ m{^(.*?)(?:#(\d+))?$});

	show_filelogs($dbh, $depotpath, $rev, \%filelog_opts);
}

sub _p4_disp_rev {
	my $low = shift;
	my $high = shift;
	$low++;
	($low == $high ? "#$high" : "#$low,#$high");
}

# this function converts a perforce type bitmap to a text type.  I
# didn't try very hard to understand the layout or make this function
# very clever, I just cared about the types I saw in my own test
# repository.
use constant P4_TYPE_EXEC => 0b10_0000_0000;
use constant P4_TYPE_KORRUPT => 0b10_0000;  # korrupt on checkout
sub _p4_type {
	my $type = shift;
	my @supp;
	if ($type & 4096) {
       		$type ^= 4096;
		push @supp, "+w";
	}
	return join "",
		($type ==              0 ? "text"     :
		$type ==   0b1_0000_0011 ? "binary"   :
		$type ==   0b1_0000_0001 ? "ubinary"  :
		$type ==    P4_TYPE_EXEC ? "xtext"    :
		$type == P4_TYPE_KORRUPT ? "ktext"    :
		$type ==  0b10_0010_0000 ? "kxtext"   :
		$type ==  0b01_0000_0000 ? "binary+D" :
		$type ==0b1101_0000_0011 ? "apple"    :
		$type == 0b100_0000_0000 ? "symlink"  : "xxx-".sprintf("%b",$type)),
		@supp;
}

sub _p4_changelog {
	my $row = shift;
	my $o = shift || {};
	my @rv = ("$row->{change} ",
		  (defined $row->{change_type}
		   ? ($row->{change_type}, " ") : ()),
		  "on $row->{when} by $row->{who_user}",
		  "\@$row->{who_host}",
		  ($row->{file_type}
		   ? (" (",_p4_type($row->{file_type}), ")") : ()));

	if ( $o->{show_desc} || $o->{show_some_desc} ) {
		push @rv, "\n\n",
			(map { "\t$_\n" }
			 split /\n/,
			 ( $o->{show_some_desc}
			   ? substr $row->{description}, 0, 250
			   : $row->{description} ));
		push @rv, "\n";
	}
	else {
		my $short = $row->{short_desc};
		$short =~ s{\s}{ }g;
		push @rv, " '$short'\n";
	}
	@rv;
}

sub show_filelogs {
	my $dbh = shift;
	my $depotpath = shift;
	my $rev = shift;
	my $o = shift;

	# build the query.
	my @placeholders;

	my $time_fmt = 'YYYY/MM/DD';
	if ( $o->{show_time} ) {
		$time_fmt .= ' HH:MI:SS';
	}
	push @placeholders, $time_fmt, $depotpath;

	my $revision_clause = '';
	if ( $rev ) {
		$revision_clause = 'and revision <= ?';
		push @placeholders, $rev;
	}

	my $limit_clause = '';
	if ( $o->{maxRevs}) {
		$limit_clause = "limit ?";
		push @placeholders, $o->{maxRevs};
	}

	my $x = $o->{select_extra}||"";

	my $output = $o->{output_func};
	if ( !$output ) {
		$output = sub {
			my $row = shift;
			my $ii = shift;
			print "... #$row->{revision} change ",
				_p4_changelog($row, $o);
			while ( my $i = $ii->() ) {
				my $other;
				my ($low, $high);
				if ( $i->{subject} eq $row->{depotpath} ) {
					$other = $i->{object};
					($low, $high) = @{$i}{
						qw(object_minrev
						   object_maxrev)};
				}
				else {
					$other = $i->{subject};
					($low, $high) = @{$i}{
						qw(subject_minrev
						   subject_maxrev)};
				}
				my $disp_rev = _p4_disp_rev($low, $high);
				print "... ... $i->{int_title} $other",
					"$disp_rev\n";
			}
		};
	}
	my $oh = $o->{output_header} || sub {
		print "$depotpath\n";
	};
	my $long_desc = "";
	my $desc_join = "";
	if ( $o->{show_desc} or $o->{show_some_desc} ) {
		$long_desc = "\tchange_desc.description,";
		$desc_join = "\tleft join change_desc\n"
			."\t\tusing (change_desc_id)";
	}

	my $sql = <<SQL;
select
$long_desc
$x
	to_char(to_timestamp(change_time), ?) as when,
	*
from
	revcx_path
$desc_join
where
	depotpath = ? $revision_clause
order by
	revision desc
$limit_clause
SQL
	whisper "running: $sql";
	my $query = $dbh->prepare($sql);
	$query->execute(@placeholders);

	my $integed_fetch = $dbh->prepare(<<SQL);
select
	integed.*,
	int_type.title as int_title
from
	integed
	inner join int_type
		using (int_type)
where
	(subject = ? and subject_maxrev = ?)
	-- or (object = ? and object_maxrev = ?)
order by
	object, object_maxrev desc
SQL
	$oh->();
	while ( my $row = $query->fetchrow_hashref ) {
		my $executed;
		my $int_rows_iter = sub {
			$integed_fetch->execute
				($depotpath, $row->{revision})
					unless $executed++;
			$integed_fetch->fetchrow_hashref;
		};
		$output->($row, $int_rows_iter);
	}
}

sub do_integrated {
	my $dbh = shift;
	my %integed_opts;
	{
		@ARGV = @_;
		getopt("r" => \$integed_opts{reverse},
		      );
		@_ = @ARGV;
	}

	my $depotpath = shift;

	show_integes($dbh, $depotpath, \%integed_opts);
}

sub show_integes {
	my $dbh = shift;
	my $depotpath = shift;
	my $o = shift;

	my $which = "subject";
	if ( $o->{reverse} ) {
		$which = "object";
	}

	my $output = $o->{output} || sub {
		my $row = shift;
		my $subj_dr = _p4_disp_rev
			($row->{subject_minrev}, $row->{subject_maxrev});
		my $obj_dr = _p4_disp_rev
			($row->{object_minrev}, $row->{object_maxrev});

		print "$row->{subject}$subj_dr - ",
			("$row->{int_title} $row->{object}$obj_dr",
			 "\n");
	};

	my $sth = $dbh->prepare(<<SQL);
select
	integed.*,
	int_type.title as int_title
from
	integed
	inner join int_type
		using (int_type)
where
	$which = ?
order by
	object,
	object_minrev,
	subject_maxrev
SQL

	$sth->execute($depotpath);

	while ( my $row = $sth->fetchrow_hashref ) {
		$output->($row);
	}
}

sub do_describe {
	my $dbh = shift;

	my $write;
	my %desc_opts;
	{
		@ARGV = @_;
		getopt( "l" => \$desc_opts{long},
			"w" => \$write,
		      );
		@_ = @ARGV;
	}
	abort "no change passed to describe" if !@_;

	my $change = shift;
	if ( $change !~ m{^\d+$} ) {
		abort "'$change' is not a valid change number";
	}

	if ( !$write ) {
		$desc_opts{add_change_branch} = sub {
			my $branch = shift;
			my $change = shift;
			print "action: add change_branch for $branch\@$change\n";
		};
		$desc_opts{add_parent} = sub {
			my $row = shift;
			print "action: add $row->{parent_branchpath}\@"
				.("$row->{parent_change} as a parent of"
				  ." $row->{branchpath}\n");
		};
	}

	change_stats($dbh, $change, \%desc_opts);
}

sub derive_branch_src {
	my $path = shift;
	my $PcO = shift;
	my $callback = shift;
	my $branches = $PcO->{branch};
	if ( keys %$branches > 1 ) {
		$DB::single = 1;
		print "multiple branch sources for $path, erp!";
		print "Too many parents: @{[ map{ $_||$path } keys %$branches ]}\n";
	}
	else {
		my ($source_path, $info) = %$branches;
		$callback->({ branchpath => $path,
			      parent_branchpath => $source_path,
			      parent_change => $info->{int_change},
			      manual => 0,
			    });
	}
}

sub derive_extra_parents {
	my $path = shift;
	my $PcO = shift;
	my $POc = shift;
	my $prev = shift;
	my $callback = shift;

	my $root_changes = delete $POc->{""};

	$callback->({ branchpath => $path,
		      parent_branchpath => $path,
		      parent_change => $prev->{$path},
		      manual => 0 });

	# warn for octopus and evil merges
	if ( keys %$POc > 1 ) {
		print( ($root_changes ? "Shoggoth" : "Octopus")
		       ." merge of: @{[map { $_||$path} keys %$POc]}\n" );
	}
	elsif ( $root_changes ) {
		print "This is an EVIL merge\n";
	}

	# add all parents
	while ( my ($source_path, $chg_info) = each %$POc ) {

		my ($all_headrev, $none_unseen, $change );
		my $change = max map { $_->{int_subj_max_change}||-1 }
			map { @$_ } map { values %$_ }
				values %$chg_info;
		if ( !grep { !$_->{int_subj_headrev} }
		     map { @$_ } map { values %$_ }
		     values %$chg_info ) {
			$all_headrev = 1;
			$change = $prev->{$source_path};
		}

		$callback->({ branchpath => $path,
			      parent_branchpath => $path,
			      parent_change => $change,
			      all_headrev => $all_headrev,
			      none_unseen => $none_unseen,
			      manual => 0 });
	}
}

use POSIX qw(strftime);

sub change_stats {
	my $dbh = shift;
	my $change = shift;
	my $o = shift;
	my $csv;

	# ok, so there are a few internal callbacks in this function.
	# the first gets the description of the change - roughly
	# change_desc
	my $show_header = $o->{show_header} || sub {
		my $d = shift;
		print "Change $d->{change} by $d->{who_user}\@",
			("$d->{who_host} on ",
			 strftime("%Y/%m/%d %H:%M:%S",
				   localtime($d->{change_time})));
		if ( $o->{compact} ) {
			print " '".substr($d->{description},0,40)."'\n";
		}
		else {
			print "\n\n";
			print map { "\t$_\n" } split "\n", $d->{description};
			print "\n";
		}
	};

	# this one gets called with the list of branches
	my %prev_paths;
	my %curr_paths;
	my $prepare_paths = sub {
		my $row = shift;
		if ( $row->{change} == $change ) {
			$curr_paths{$row->{branchpath}}++;
		}
		else {
			$prev_paths{$row->{branchpath}} = $row->{change};
		}
		$o->{prepare_path}->($row) if $o->{prepare_path};
	};

	# this one receives any pre-existing change_parents rows
	my %parents;
	my $change_parents = sub {
		my $row = shift;
		push @{ $parents{$row->{branchpath}} ||= [] }, $row;
		$o->{change_parents}->($row) if $o->{change_parents};
	};

	# this one gets change_detail 'rows'
	my $show_change_detail = $o->{show_change_detail} = sub {
		my $path = shift;
		my $details = shift;
		my $info = shift;
		print "On ".($info->{new}?"new ":"")."path $path,\n";
		while (my ($change_title, $by_int_path)
		       = each %$details) {
			for my $int_path (sort keys %$by_int_path) {
				my $c_d = $by_int_path->{$int_path};
				print "\t", $change_title, " ";
				if ( $int_path ) {
					print "from $int_path ("
						.($c_d->{int_headrev}
						  ?"head":
						  $c_d->{int_change})."), ";
				}
				print scalar(@{$c_d->{revcxs}})," file(s)";
				print "\n";
				if ( $o->{long} ) {
					for my $c ( @{$c_d->{revcxs}} ) {
						show_chg($path, $c);
					}
				}
			}
		}
		print "\n";
	};

	goto no_header if !ref $show_header;
	my $sth = $dbh->prepare(<<SQL);
select
	*
from
	change_desc
	inner join change
		using (change_desc_id)
where
	change.change = ?
SQL
	$sth->execute($change);
	my $row = $sth->fetchrow_hashref;
	if ( !$row ) {
		say "no such change $change";
		return;
	}
	if ( !$row->{closed} ) {
		moan "change $change was not closed";
	}
	$show_header->($row);
	$sth->finish;

no_header:
	$sth = $dbh->prepare(<<SQL);
select
	branchpath,
	max(change) as change
from
	change_branches
where
	change < ?
group by
	branchpath
union
select
	branchpath,
	change
from
	change_branches
where
	change = ?
SQL
	$sth->execute(($change) x 2);
	while ( my $row = $sth->fetchrow_hashref ) {
		$prepare_paths->($row);
	}
	$sth->finish;

	my $know_parents;
	$sth = $dbh->prepare(<<SQL);
select
	*
from
	change_parents
where
	change = ?
order by
	manual desc
SQL
	$sth->execute($change);
	while ( my $row = $sth->fetchrow_hashref ) {
		$know_parents++;
		$change_parents->($row);
	}
	$sth->finish;

	$sth = $dbh->prepare(<<SQL);
select
	*,
	change_type.title as change_title
from
	revcx_integed
	inner join change_type
		using (change_type)
where
	change = ?
order by
	depotpath
SQL
	$sth->execute($change);
	my (@revcxs);

	my $path_re;
	my %paths = (%prev_paths, %curr_paths);
	my $add_path = sub {
		if ( @_ ) {
			if ( grep { (!$_[1] or defined $paths{$_[0]})
					    and m{^\Q$_[0]\E/} } keys %paths ) {
				return undef;
			}
			$paths{(shift)} ||= undef;
		}
		if ( keys %paths ) {
			my $re = join("|",map{"\Q$_\E"} keys %paths);
			$path_re = qr{$re};
		}
		else {
			$path_re = qr{(?!.)};
		}
	};
	$add_path->();

	# now, set about grouping the revcx_integed stuff by branch,
	# change type and integrated branch
	while ( my $row = $sth->fetchrow_hashref ) {
		# try to figure out the branch root by looking for top
		# level changes, or integrates from other paths.  Hope
		# no integrations are happening between files with
		# different names!
		my $path = $row->{depotpath};
		unless ( $path =~ m{^($path_re)/} ) {
			# new path, add to paths
			$path =~ s{/[^/]*$}{};
			if ( $row->{int_obj} and
			     $row->{int_obj} =~ m{^($path_re)/} ) {
				$path = substr $row->{depotpath}, 0,
					length($row->{depotpath}) -
					(length($row->{int_obj}) -
					 length $1);
			}
			elsif ( $row->{int_obj} ) {
				$path = diff_right($row->{int_obj},
						   $row->{depotpath});
				moan "using diff_right($row->{int_obj}, $row->{depotpath}) for change branch path (=$path)";
			}
			else {
				mutter "adding new branchpath $path because I saw $row->{depotpath}";
			};
			$add_path->($path, 1);
		}
		push @revcxs, $row;
		$o->{revcxs}->($row, $path) if $o->{revcxs};
	};

	# paranoia - the above was a quick sweep only.  remove the paths
	# that shadow each other.
	my @gonners = grep { m{^($path_re)/} && !$paths{$_} }
		keys %paths;
	if ( @gonners ) {
		for my $gonner ( @gonners ) {
			if ( my @x = grep m{^\Q$_\E/}, keys %prev_paths,
			     keys %curr_paths ) {
				# this is a bug - the $add_path closure
				# shouldn't let these through
				die "$gonner shadows branches: @x";
			}
			delete $paths{@gonners};
			moan "ignoring changes on path $gonner\@$change";
		}
		$add_path->();
	}

	# ok, now group them by path and integration branch, and change type
	my (%by_path_chg_obj, %by_path_obj_chg);
	my @change_details;
	for my $row ( @revcxs ) {
		{ no warnings;
		#whisper "row: $row->{depotpath} $row->{change_title} $row->{int_obj}";
	  }
		my ($path) = ($row->{depotpath} =~ m{^($path_re)/})
			or die "got confused.  depotpath = "
				.("$row->{depotpath}, looking "
				  ."for $path_re");
		my $op = "";
		if ( $row->{int_obj} &&
		     $row->{int_obj} =~ m{^($path_re)/} ) {
			$op = $1;
		}
		elsif ( $row->{int_obj} ) {
			$op = diff_right($row->{depotpath},
					 $row->{int_obj});
			moan "using diff_right($row->{depotpath}, $row->{int_obj}) for integration branchpath (=$op)";
			while ( !$add_path->($op) ) {
				($op) = $row->{int_obj} =~ m{^(\Q$op\E/[^/]+)};
				moan "no, that'll shadow something, use $op instead";
			};
		}
		my $c_d = $by_path_chg_obj{$path}{$row->{change_title}}{$op}
			||= do {
			my $change_detail =
				{ branchpath => $path,
				  change_title => $row->{change_title},
				  int_branch => $op,
				  revcxs => []
				};
			push @change_details, $change_detail;
			$change_detail;
		};
		$by_path_obj_chg{$path}{$op}{$row->{change_title}} = $c_d;
		push @{$c_d->{revcxs}}, $row;
	}

	# within those groups, see if the changes are all headrev or
	# their max_change
	for my $c_d ( @change_details ) {
		$c_d->{int_change} =
			max map { $_->{int_subj_max_change}||-1 }
				@{$c_d->{revcxs}};
		$c_d->{int_headrev} =
			!(grep {
				!$_->{int_subj_headrev}
			} @{$c_d->{revcxs}});
	}

	my $show_parent = sub {
		my $row = shift;
		print "Parent: ",
			(($row->{ref} ? "$row->{ref}"
			  : ("$row->{parent_branchpath}\@"
			     ."$row->{parent_change}")),
			 ((!$row->{ref}&&$row->{manual})
			  ? " (manual)" : ""),
			 "\n");
	};

	my $add_parent_sth;
	my @change_parent_cols = qw(branchpath change parent_branchpath
				    parent_change ref manual all_headrev
				    none_unseen);
	my $add_parent = $o->{add_parent} ||= sub {
		my $row = shift;
		$show_parent->($row);

		$add_parent_sth ||= $dbh->prepare(<<SQL);
insert into change_parents
	(${\(join ", ", @change_parent_cols)})
values
	(${\(join ", ", ("?") x @change_parent_cols)})
SQL
		mutter "saving new parent: "
			.join(",",map{defined($_)?$_:"NULL"}@_)
				if $VERBOSE >= 1;
		my @row;
		for my $col ( @change_parent_cols ) {
			push @row, $row->{$col};
		}
		$add_parent_sth->execute(@row);
	};
	my $add_change_branch_sth;
	my $add_change_branch = $o->{add_change_branch} ||= sub {
		$add_change_branch_sth ||= $dbh->prepare(<<SQL);
insert into change_branches
	(branchpath, change)
values (?, ?)
SQL
		mutter "saving new branch path: @_";
		$add_change_branch_sth->execute(@_);
	};

	my @ncp;
	my $new_change_parents = sub {
		my $row = shift;
		$row->{change} = $change;
		push @ncp, $row;
		$change_parents->($row);
	};
	# derive extra parents if necessary
        for my $path ( sort keys %by_path_chg_obj ) {
		$add_change_branch->($path, $change)
			unless $curr_paths{$path};
		my $PcO = $by_path_chg_obj{$path};
		my $POc = $by_path_obj_chg{$path};
		my $parents = $parents{$path}||[];
		if ( grep { $_->{manual} } @$parents ) {
			$parents = [ grep { $_->{manual} } @$parents ];
		}
		if ( $know_parents ) {
			for ( @$parents ) {
				$show_parent->($_);
			}
		}

		my $info = {};
		if ( !exists $prev_paths{$path} ) {
			$info->{new} = 1;
			if ( exists $PcO->{integrate} or
			     exists $PcO->{delete} or
			     exists $PcO->{edit}
			   ) {
				moan "new path $path@\$change has non-new changes, odd";
				$info->{invalid} = 1;
			}
			if ( !$know_parents and $PcO->{branch} ) {
				# derive the branch's parent
				derive_branch_src $path, $PcO
					=> $new_change_parents;
			}
		}
		elsif ( grep { length } keys %$POc ) {
			# some inter-branch action.
			if ( !$know_parents ) {
				derive_extra_parents $path, $PcO, $POc,
					\%prev_paths,
					=> $new_change_parents;
			}
		}
		else {
			if ( !$know_parents ) {
				$new_change_parents->
					({ branchpath => $path,
					   parent_branchpath => $path,
					   manual => 0,
					   parent_change => $prev_paths{$path},
					 });
			}
		}
		$show_change_detail->($path, $PcO, $info);
		if ( !$know_parents ) {
			for my $row (@ncp) {
				$add_parent->($row);
			}
			@ncp=();
		}
	}
}

sub show_chg {
	my $path = shift;
	my $chg = shift;

	print "\t\t";
	my $filename = substr $chg->{depotpath}, length $path;
	if ( $chg->{int_obj_title} ) {
		my $odr = _p4_disp_rev
			($chg->{int_obj_min}, $chg->{int_obj_max});
		my $sdr = _p4_disp_rev
			($chg->{int_subj_min}, $chg->{int_subj_max});
		if ( $sdr =~ m{,} ) {
			$sdr .= " ($chg->{int_subj_min_change},"
				."$chg->{int_subj_max_change})";
		}
		else {
			$sdr .= " ($chg->{int_subj_max_change})";
		}
		if ( $chg->{int_subj_headrev} ) {
			$sdr .= "(HEAD)";
		}

		print "...$filename: $sdr $chg->{int_obj_title} "
			.($odr ne "#$chg->{revision}" ? "$odr" : "us"),
				"\n";
	}
	else {
		print "...$filename\n";
	}
}

sub make_cc {  # only CC's eez tasting like theez
	my $self =
		{ path => shift,
		  change_title => shift,
		  changes => shift,
		  obj_path => shift,
		  obj_change => shift,
		  obj_headrev => shift };

	
}

sub diff_right {
	my $left = shift;
	my $right = shift;
	my $done = 0;
	while (!$done) {
		my ($last_component) = $left =~ m{(/[^/]+)$};

		if (!defined($last_component) or
			$right !~ s{\Q$last_component\E$}{}) {

			$done = 1;
		}
		else {
			$left =~ s{\Q$last_component\E$}{};
		}
	}
	return $right;
}

sub do_changes {
	my $dbh = shift;
	my %changes_opts;
	{
		@ARGV = @_;
		getopt("l" => \$changes_opts{show_desc},
		       "L" => \$changes_opts{show_some_desc},
		       "m=i" => \$changes_opts{maxRevs},
		      );
		@_ = @ARGV;
	}

	my $pathspec = shift;
	my ($depotpath, $minRev, $maxRev)
		= ($pathspec =~ m{^(.*?)(?:#(\d+)(?:,#?(\d+))?)?$})
			if $pathspec;
	$maxRev = $minRev if !$maxRev;

	show_changes($dbh, $depotpath, $minRev, $maxRev, \%changes_opts);
}

sub show_changes {
	my $dbh = shift;
	my $depotpath = shift;
	my $minRev = shift;
	my $maxRev = shift;
	my $o = shift;

	my $output = $o->{output} ||= sub {
		my $row = shift;
		print "Change ", _p4_changelog($row, $o);
	};

	# build the query.  some code duplicated from show_filelogs,
	# CBA fixing for now...
	my @placeholders;

	my $time_fmt = 'YYYY/MM/DD';
	if ( $o->{show_time} ) {
		$time_fmt .= ' HH:MI:SS';
	}
	push @placeholders, $time_fmt;

	my @filters;

	my $long_desc = "";
	my $desc_join = "";
	if ( $o->{show_desc} or $o->{show_some_desc} ) {
		$long_desc = "\tchange_desc.description,";
		$desc_join = "\tleft join change_desc\n"
			."\t\tusing (change_desc_id)";
	}

	my $limit_clause = '';
	if ( $o->{maxRevs}) {
		$limit_clause = "limit ?";
		push @placeholders, $o->{maxRevs};
	}

	my $revcx_join = "";
	if ( $depotpath ) {
		$revcx_join = "\tinner join revcx\n"
			.("\t\ton (revcx.change = change.change and\n"
			  ."\t\t\trevcx.depotpath = ?)");
		push @placeholders, $depotpath;
	}

	if ( $maxRev ) {
		if ( $minRev ) {
			push @filters, 'revision between ? and ?';
			push @placeholders, $minRev, $maxRev;
		}
		else {
			push @filters, 'revision <= ?';
			push @placeholders, $maxRev;
		}
	}

	my $where_clause = "";
	if ( @filters ) {
		$where_clause = "where\n\t".join("\nand\t", @filters);
	}

	my $sql = <<SQL;
select
$long_desc
	to_char(to_timestamp(change_time), ?) as when,
	change.*
from
	change
$revcx_join
$desc_join
$where_clause
order by
	change.change desc
$limit_clause
SQL
	if ( $VERBOSE>1) {
		say "querying changes with: $sql";
		say "placeholders: (".join(", ", @placeholders).")";
	}
	my $sth = $dbh->prepare($sql);
	$sth->execute(@placeholders);
	while ( my $row = $sth->fetchrow_hashref ) {
		$output->($row);
	}
}

sub max_change {
	my $dbh = shift;
	my ($change) = map { @$_ } @{ $dbh->selectall_arrayref(<<SQL) };
select
	max(change)
from
	change
SQL
	return $change;
}

sub get_rcs {
	my $rcs_file = shift;
	my $rcs_revision = shift;
	return undef if !$rcs_file;
	$rcs_file =~ s{^//}{};
	my $cmd;
	if ( -f "$rcs_file,d/$rcs_revision" ) {
		$cmd = "cat '$rcs_file,d/$rcs_revision'";
	}
	elsif ( -f "$rcs_file,d/$rcs_revision.gz" ) {
		$cmd = "zcat '$rcs_file,d/$rcs_revision'";
	}
	else {
	#    In fact I'd be highly tempted to parse the RCS file
	#    directly, as it might be significantly faster to hold
	#    the latest version of each RCS file in memory, as we work
	#    backwards through the revisions and construct new
	#    versions of it for feeding to fast-import.  Consider an
	#    RCS file with many revisions; getting all revisions out
	#    with `co -p` will be O(N^2) but directly will be O(N)
		$cmd = "co -q -p$rcs_revision -kb '$rcs_file'";
	}
	open CMD, "-|", "$cmd" or die $!;
	binmode CMD;
	local($/) = \((stat CMD)[11]||4096);
	my @data = <CMD>;
	close CMD;
	join "", @data;
}

sub do_print {
	my $dbh = shift;
	my %print_opts;
	{
		@ARGV = @_;
		getopt("Q" => \$print_opts{quiet},
		       "c=i" => \$print_opts{change},
		       "a" => \$print_opts{all},
		      );
		@_ = @ARGV;
	}

	my $pathspec = shift;
	my ($depotpath, $rev)
		= ($pathspec =~ m{^(.*?)(?:#(\d+))?$})
			if $pathspec;

	show_file($dbh, $depotpath, $rev, \%print_opts);
}

sub show_file {
	my $dbh = shift;
	my $depotpath = shift;
	my $rev = shift;
	my $o = shift;
	my $change = $o->{change};

	if ( !$rev ) {
		$change ||= max_change($dbh);
	}

	my @where;
	my @pl;

	push @where, "depotpath = ?";
	push @pl, $depotpath;

	if ( !$rev ) {
		push @where, "change <= ?";
		push @pl, $change;
	}
	else {
		push @where, "revision = ?";
		push @pl, $rev;
	}

	my $where = join " and ", @where;
	my $limit = ($o->{all} ? "" : "limit 1");
	my $sth = $dbh->prepare(<<SQL);
select
	revision,
	rev_change_type,
	change,
	ct.title as change_title,
	file_type,
	rcs_file,
	rcs_revision
from
	rev
	inner join change_type ct
		on (ct.change_type = rev.rev_change_type)
where
	$where
order by
	revision desc
$limit
SQL
	$sth->execute(@pl);
	my $output = $o->{output} || sub {
		my $row = shift;
		unless ( $o->{quiet} ) {
			print "$depotpath#$row->{revision} - "
				.("$row->{change_title} change "
				  .$row->{change}." ("._p4_type($row->{file_type})
				  .")\n");
		}
		my $file_data = get_rcs($row->{rcs_file},
					$row->{rcs_revision});
		binmode STDOUT, ":raw";
		print $file_data;
	};
	while ( my $row = $sth->fetchrow_hashref ) {
		$output->($row);
	}
	$sth->finish;
}

sub do_ls_tree {
	my $dbh = shift;
	my %ls_tree_opts;
	{
		@ARGV = @_;
		getopt("l" => \$ls_tree_opts{length},
		       "b" => \$ls_tree_opts{blob},
		       "r" => \$ls_tree_opts{recurse},
		       "name-only" => \$ls_tree_opts{name_only},
		       "abbrev=i" => \$ls_tree_opts{abbrev},
		      );
		@_ = @ARGV;
	}

	my $pathspec = shift;
	my ($depotpath, $change)
		= ($pathspec =~ m{^(.*?)(?:@(\d+))?$})
			if $pathspec;
	if ( @_ or !$depotpath ) {
		abort "expected depot path/change (only)";
	}
	$change ||= max_change($dbh);

	show_files($dbh, $depotpath, $change, \%ls_tree_opts);
}

sub sha1_blob_ref {
	my $data_ref = shift;
	my $sha1 = Digest::SHA1->new;
	my $l = length($$data_ref);
	$sha1->add("blob $l\0");
	$sha1->add($$data_ref);
	return lc($sha1->hexdigest);
}

sub show_files {
	my $dbh = shift;
	my $path = shift;
	my $change = shift;
	my $o = shift;

	my $output = $o->{output} ||= sub {
		my $row = shift;
		(my $relative = $row->{depotpath})
			=~ s{^\Q$path\E/?}{};
		if ( $o->{name_only} ) {
			print "$relative\n";
		}
		else {
			my ($l, $b);
			$b = $row->{blobid};
			if ( $o->{length} or
			     ($o->{blob} and !$row->{blobid}) ) {
				my $blob_data =
					get_rcs($row->{rcs_file},
						$row->{rcs_revision});
				if ( $o->{length} ) {
					$l = length $blob_data;
				}
				if ( $o->{blob} ) {
					# pass by ref only needed on
					# perl <5.8.1
					$b = sha1_blob_ref(\$blob_data);
				}
			}
			my $a = $o->{abbrev} || 40;
			# don't show trees yet
			printf( "%6o %s %s".($o->{length}?" %7d":"")
				."\t%s\n",
				($row->{file_type} & P4_TYPE_EXEC
				 ? 0100755 : 0100644),
				"blob",
				($o->{blob}
				 ? substr($b,0,$a)
				 : substr($row->{revision_md5},0,$a)),
				($o->{length} ? ($l) : ()),
				$relative);
		}
	};

	my (@where, @pl);

	push @where, "change = ?";
	push @pl, $change;

	if ( $o->{recurse} ) {
		push @where, 'depotpath like ?';
		push @pl, "$path/%";
	}
	else {
		push @where, 'depotpath ~ ?';
		push @pl, "$path(/[^/]*)?\$";
	}

	my $join_clause = "";
	if ($o->{blob} || $o->{marks}) {
		$join_clause = <<SQL;
	left join rev_marks
		using (depotpath, revision)
SQL
		if ($o->{blob}) {
			$join_clause .= <<SQL;
	left join marks
		using (mark)
SQL
		}
	}

	my $where = join " and ", @where;
	my $sth = $dbh->prepare(<<SQL);
select
	*
from
	change_state cs
$join_clause
where
	$where
order by
	depotpath asc
SQL
	$sth->execute(@pl);

	while ( my $row = $sth->fetchrow_hashref ) {
		$output->($row);
	}
}

sub min_change {
	my $dbh = shift;
	my $o = shift;

	my $joins = "";
	my $having = "";

	if ( $o->{blobs} ) {
		$joins = <<SQL;
	left join rev r
		on (r.change = c.change and
			r.rev_change_type != 2)
	left join rev_marks rm
		using (depotpath, revision)
SQL
		$having = <<SQL;
having
	count(rm.mark)=0
SQL
	}
	elsif ( $o->{changes} ) {
		$joins = <<SQL;
	left join change_marks cm
		using (change)
SQL
		$having = <<SQL;
having
	count(cm.mark)=0
SQL
	}
	elsif ( $o->{branches} ) {
		$joins = <<SQL;
	left join change_branches cb
		using (change)
SQL
		$having = <<SQL;
having
	count(cb.branchpath)=0
SQL
	}

	my $sql = <<SQL;
select
	c.change
from
	change c
${joins}
where
	closed = 1
group by
	c.change
${having}
order by
	c.change asc
limit 1
SQL

	mutter "finding first change with new "
		.($o->{blobs}?"blobs":"commits");
	my $change_sth = $dbh->prepare($sql);
	$change_sth->execute;
	my $min_change;
	$change_sth->bind_col(1, \$min_change);
	$change_sth->fetch;
	$change_sth->finish;

	return $min_change;
}

sub parse_export_opts {
	my $dbh = shift;
	my $type = shift;
	my %export_opts;
	{
		@ARGV = @_;
		my @gfi_options;
		getopt("n=i" => \$export_opts{chunk_size},
		       "depth=i" => \$export_opts{depth},
		      );
		@_ = @ARGV;
	}
	if ( @_ ) {
		my $revspec = shift;
		if ( $revspec !~ m{^(\d+)(?:\.\.(\d+))?$} ) {
			abort "bad change spec '$revspec'";
		}
		@export_opts{qw(min max)} = ($1, $2);
	}
	if ( !$export_opts{min} ) {
		$export_opts{min} = min_change($dbh, { $type => 1 });
	}
	if ( $export_opts{min} and $export_opts{chunk_size} ) {
		$export_opts{max} = $export_opts{min}
			+ $export_opts{chunk_size} - 1;
	}
	if ( !$export_opts{max} ) {
		$export_opts{max} = max_change($dbh);
	}
	if ( $export_opts{min} and $export_opts{max} ) {
		$export_opts{chunk_size} = $export_opts{max}
			- $export_opts{min} + 1;
	}

	$export_opts{verbose} = $VERBOSE;
	%export_opts;
}

sub do_export_blobs {
	my $dbh = shift;

	my %export_opts = parse_export_opts($dbh, "blobs", @_);

	gfi_open(\%export_opts);

	export_blob_chunk($dbh, \%export_opts);
}

our $MARKS_FILE;

sub gfi_open {
	my $eo = shift;
	$MARKS_FILE ||= "p4raw.$$.marks";
	if ( -t STDOUT ) {
		open GFI, "|git fast-import "
			.($eo->{gfi_options}||"")
			.($eo->{depth}?"--depth=$eo->{depth} ":"")
			."--export-marks=$MARKS_FILE"
				or barf "popen gfi failed; $!";
	}
	else {
		open GFI, ">&STDOUT";
		open STDOUT, ">&STDERR";
		moan "won't be able to commit!";
		say "drop constraints on rev_marks / change_marks or "
			."this run will not be restartable";
		undef($MARKS_FILE);
	}
	binmode GFI;
}

use constant CHUNK_SIZE => 4096;

our $MARK_MIN;
our $MARK_MAX;
our $MARK;

sub gfi_get_marks {
	my $dbh = shift;
	my $count = shift;
	my ($dummy) = map { @$_ } @{ $dbh->selectall_arrayref(<<SQL) };
select nextval('gfi_mark')
SQL
	$dbh->do(<<SQL) or die $dbh->errstr;
alter sequence gfi_mark increment by $count
SQL
	($MARK_MAX) = map { @$_ } @{ $dbh->selectall_arrayref(<<SQL) };
select nextval('gfi_mark')
SQL
	$dbh->do(<<SQL) or die $dbh->errstr;
alter sequence gfi_mark increment by 1
SQL
	$MARK_MIN = $MARK_MAX - $count + 1;
	die "read nextval of $MARK_MAX" unless $MARK_MIN > 0;
	$MARK = $MARK_MIN - 1;
}

our %MARKS;
our %MARK_TYPES = qw(blob 1 commit 2 tag 3);

sub gfi_get_mark {
	my $dbh = shift;
	my $type = shift;
	unless ( $MARK_MAX and ++$MARK <= $MARK_MAX ) {
		($MARK) = map { @$_ } @{ $dbh->selectall_arrayref(<<SQL) };
select nextval('gfi_mark')
SQL
	}
	$MARKS{$MARK}=$MARK_TYPES{$type};
	$MARK;
}

sub gfi_send_blob {
	my $dbh = shift;
	my $buf = shift;
	my $l = length($$buf);
	my $mark = gfi_get_mark($dbh, "blob");
	print GFI "blob\n";
	print GFI "mark :", $mark, "\n";
	print GFI "data $l\n";
	print GFI $$buf, "\n";
	$mark;
}

sub gfi_checkpoint {
	my $dbh = shift;

	if ( !defined $MARKS_FILE ) {
		moan "don't know where the marks are going!  hope you "
			.("dropped those rev_marks / change_marks "
			  ."constraints");
		goto commit;
	}

	if ( !keys %MARKS ) {
		moan "nothing to checkpoint";
		goto commit;
	}

	my $last_mtime = (stat $MARKS_FILE)[9];
	say "Now checkpointing.";
	print GFI "checkpoint\n\n";
	GFI->flush();

	my $time;
	while ( ! -e $MARKS_FILE or
		$last_mtime and
		((stat _)[9] == $last_mtime) ) {
		sleep 1;
		$time++;
		say "waited ${time}s for $MARKS_FILE to be created";
	}

	my $insert_mark_sth = $dbh->prepare(<<SQL);
insert into marks
	(mark, commitid, blobid)
values
	(?, ?, ?)
SQL
	my $b_t = $MARK_TYPES{blob};
	my $c_t = $MARK_TYPES{commit};

	while ( keys %MARKS ) {
		my @marks = `cat $MARKS_FILE`;
		my ($t, $found, $mark, $sha1);
		for ( @marks ) {
			if (($mark, $sha1) =
			    m{^:(\d+) ([0-9a-f]{40})} and
			    ($t = delete $MARKS{$mark})
			   ) {
				$found++;
				$insert_mark_sth->execute
					($mark,
					 ($t == $c_t ? $sha1 : undef),
					 ($t == $b_t ? $sha1 : undef),
					);
			}
		}
		if ( $found ) {
			mutter "wrote $found marks to DB";
		}
		else {
			moan "still waiting for ".(scalar keys %MARKS)
				." mark(s) to appear";
			sleep 1;
		}
	}

 commit:
	$dbh->commit
		or moan "commit failed; ".$dbh->errstr;
}

sub export_blob_chunk {
	my $dbh = shift;
	my $o = shift;
	local($VERBOSE) = $o->{verbose};

	my $min_change = $o->{min};
	my $chunk_size = $o->{chunk_size};
	my $max_change = $o->{max};

	# psuedocode for blob import.
	# 1. find the lowest change which is yet to be marked as
	#    imported
	say "exporting blobs for changes $min_change .. "
		."$max_change";

	# 2. get all of the files in it
	$dbh->begin_work;
	my ($count_em_sth) = $dbh->prepare(<<SQL);
select
	count(distinct rev.depotpath),
	count(rev.revision)
from
	rev
	left join rev_marks
		using (depotpath, revision)
where
	rev.rev_change_type != 2 and
	rev_marks.mark is null and
	change between ? and ?
SQL
	$count_em_sth->execute($min_change, $min_change+$chunk_size-1);
	my ($paths, $revisions);
	$count_em_sth->bind_columns(\$paths, \$revisions);
	$count_em_sth->fetch;
	$count_em_sth->finish;
	mutter "this chunk touches $paths depot paths and $revisions"
		." distinct revisions";
	if ( !$revisions ) {
		say "no un-exported file revisions in this range";
		goto checkpoint;
	}

	# don't want to round-trip to the DB just to get sequence numbers
	# so do a special increment.  This is not transaction guarded!
	gfi_get_marks($dbh, $revisions);

	# 3. for each of those files, find all the ones without
	#    rev_blob rows
	my @files;

	# the sort method here could be improved further, but it is a
	# reasonable starting point and should mean that many
	# ancestrally related files are sent to fast-import in
	# sequence
	my $file_list = $dbh->prepare(<<SQL);
select
	rev.depotpath,
	rev.file_type,
	count(rev.revision) as num,
	min(rev.revision) as min,
	max(rev.revision) as max,
	integed.object,
	integed.object_minrev,
	integed.object_maxrev
from
	rev
	left join rev_marks
		using (depotpath, revision)
	left join integed
		on (int_type = 2 and subject = depotpath)
where
	rev.rev_change_type != 2 and
	rev_marks.mark is null and
	rev.change between ? and ?
group by
	depotpath,
	rev.file_type,
	integed.object,
	integed.object_minrev,
	integed.object_maxrev
order by
	(case when integed.object is null
		then depotpath
	else
		integed.object
	end),
	(case when integed.object is null
		then 0
	else
		1
	end);
SQL

	mutter "getting file list";
	$file_list->execute($min_change, $max_change);

	# 4. for each of those files, find *all* of the branched
	#    versions of it, and send them all at once to
	#    git-fast-import, entering rev_marks rows for them as we
	#    send marks to git-fast-import
	#    It's quite important to do all the revisions of a file at
	#    once, otherwise fast-import will not be able to make
	#    on-the-fly deltas and the repo will become gigabytes.
	#
	#    note "p4 print" is not required; can just use rcs
	#    directly by looking at the "rev" table; it has a rcs
	#    filename and revision that quite adequately refers to an
	#    rcs version.  So you can just collect
	#    `co -p1.2 -kb depot/mg.c` (eg), get its length, confirm
	#    the MD5, and then feed to git-fast-import using the
	#    "mark" functionality, perhaps marking it with the MD5
	#    or depotpath/revision.  Then when the fast-import
	#    "checkpoint" command is issued we will get back the
	#    GIT-SHA1 values.
	my $list_rcs_revs = $dbh->prepare(<<SQL);
select
	revision,
	rcs_file,
	rcs_revision,
	rev_change_type,
	revision_md5
from
	rev
	left join rev_marks rm
		using (depotpath, revision)
where
	rev_change_type != 2 and
	depotpath = ?	and
	rev.revision between ? and ? and
	rm.mark is null
SQL

	my $insert_rev_mark_row = $dbh->prepare(<<SQL);
insert into
	rev_marks (depotpath, revision, mark)
values
	(?, ?, ?)
SQL

	mutter "now exporting file images";
	my $tpb;
	if ( $o->{verbose} >= 0 ) {
		$tpb  = eval { Term::ProgressBar->new
				({ count => $revisions,
				   ETA => "linear",
				 }) };
	}
	my $done_revisions = 0;
	my $next_update = 1;
 	my $delete_md5 = '00000000000000000000000000000000';
	my %seen;
	my %md5_mark;
	while ( my $row = $file_list->fetchrow_hashref ) {

		my $depotpath = $row->{depotpath};
		$list_rcs_revs->execute(@{$row}{qw/depotpath min max/});
		$list_rcs_revs->bind_columns
			(\(my ($rev, $rcs_file, $rcs_revision,
			       $change_type, $md5)));

		while ( $list_rcs_revs->fetch ) {
			next if $seen{md5_hex("$depotpath#$rev")}++;

			my $has_md5 = $md5 ne $delete_md5;
			my $lc_md5 = lc($md5);
			my $mark;
			if ( $has_md5 and $md5_mark{$lc_md5} ) {
				$mark = $md5_mark{$lc_md5};
				goto record;
			}

			my $contents = get_rcs($rcs_file, $rcs_revision);

			my $found_md5;
			# don't do the md5 check on +k files
			if ( $has_md5 and
			     !($row->{file_type}&P4_TYPE_KORRUPT)
			   ) {
				$found_md5 = lc(md5_hex($contents));
				if ( $found_md5 ne $lc_md5 ) {
					# we fall over in a screaming heap
					die("MD5 mismatch on $depotpath"
					    ."#$rev ($rcs_file "
					    ."$rcs_revision); "
					    ."$found_md5 from co -p, "
					    ."$md5 in DB");
				}
			}

			$mark = gfi_send_blob($dbh, \$contents);
			$md5_mark{$lc_md5||$found_md5}
				= $mark if $has_md5 or $found_md5;
		record:
			eval {
				local($dbh->{PrintError}) = 0;
				$insert_rev_mark_row->execute
					($depotpath, $rev, $mark);
			};
			die "error inserting ($depotpath#$rev => $mark); $@"
				if $@;
			$done_revisions++;
		}
		if ( $tpb and $done_revisions >= $next_update ) {
			$next_update = $tpb->update($done_revisions);
		}
	}
	$tpb->update($done_revisions) if $tpb;
	$file_list->finish;

 checkpoint:
	unless ( $o->{no_checkpoint} ) {
		gfi_checkpoint($dbh);
	}
}

sub do_find_branches {
	my $dbh = shift;

	# "close enough" :)
	my %export_opts = parse_export_opts($dbh, "branches", @_);

	say "finding branches for $export_opts{chunk_size} changes";
	for my $change ( $export_opts{min} .. $export_opts{max} ) {
		change_stats($dbh, $change,
			     { compact => 1,
			     },
			    );
	}
}

	# 5. finally, we have rev_blob rows for all of the files
	#    in this version, so send a tree and/or commit object
	#    (though it might be easier to use
	#    `git update-index --index-info`).  Information from the
	#    integed table should probably go into the commit message,
	#    where it is not redundant (which is a difficult criterion
	#    to nail down for sure!)

sub do_export_commits {
	my $dbh = shift;
}

#=====================================================================
#  MAIN SECTION STARTS HERE
#=====================================================================
getopt("script|s" => \$SCRIPT_MODE);
if (! -t STDOUT) {
	$SCRIPT_MODE = 1;
}

binmode STDOUT, ":utf8";

my $action = shift;
abort "no action given" unless $action;
$action =~ s{-}{_}g;
abort "bad action/argument `$action'" unless defined &{"do_$action"};

my $dbh = DBI->connect("dbi:Pg:") or barf "DBI connect failed";
$dbh->{RaiseError}=1;
$dbh->{PrintError}=1;
$dbh->{AutoCommit}=1;

&{"do_$action"}($dbh, @ARGV);

__END__

=head1 NAME

git-p4raw - git fast-import exporter for RAW perforce repositories

=head1 SYNOPSIS

 # import commands:
 git-p4raw init     # to return here: git-p4raw drop
 git-p4raw load
 git-p4raw check
 git-p4raw graft CHANGE commit-ish  # configure manual reparenting
 git-p4raw import   # to return here: git-p4raw truncate

 # query commands:
 git-p4raw { find-change N | describe N | integrated DEPOTPATH
           | filelog DEPOTPATH | changes [ -l ] [ file[#range] ]
           | annotate PATH }

=head1 DESCRIPTION

B<git-p4raw> is a tool for importing perforce repositories into git,
and querying the legacy metadata in perpetuity.

It assumes you have the raw Perforce repository lying around, and does
not require the perforce server.  All you need are the C<checkpoint>,
C<journal> and RCS files.  If you want to specify things like where
the target git repository is, or which Postgres database to use, this
is configured in the normal way with environment variables
(C<GIT_DIR>, C<PGDATABASE>, etc).

=head1 COMMAND LINE OPTIONS

=over

=item B<-s, --script>

Instead of printing human-readable output, print something easily
parsable.  Automatically selected if standard output is not a terminal.

=item B<-h, --help>

Display a program usage screen and exit.

=item B<-V, --version>

Display program version and exit.

=item B<-v, --verbose>

Verbose command execution, displaying things like the
commands run, their output, etc.

=item B<-q, --quiet>

Suppress all normal program output; only display errors and
warnings.

=item B<-d, --debug>

Display output to help someone debug this script, not the
process going on.

=back

=head1 COMMANDS

=head2 IMPORT OPERATION

=over

=item C<git-p4raw init>

The first command, C<init>, initialises the database tables and git
repository.

=item C<git-p4raw drop>

C<drop> can be used to drop all database tables and destroy the git
repository, in a somewhat molly-guarded fashion.

=item C<git-p4raw load [filename...]>

The second command, C<load>, loads all of the Perforce data from the
C<checkpoint.NNN.gz> and C<journal> files which it expects to find in
the current directory (the root of the Perforce import).  You can
optionally specify which files to load from.

=item C<git-p4raw check>

This third command, C<check>, adds constraints to the database schema
and looks for errant data conditions.

If any of these constraints fail, and don't prompt to correct the data
to your satisfaction, you should investigate what is going on and
correct the data before proceeding.  The code makes assumptions that
these constraints hold.

When the command asks questions, the answers will be written to a file
called C<p4raw-extra-x.asv> in the Perforce checkpoint format (and
automatically re-loaded if you run C<load> again without arguments).

(TODO - some inconsistencies such as missing integration records are
not dealt with, and may require manual patching or use of the C<graft>
command, below)

=item C<git-p4raw graft { -d change | [ -a ] change change ... }> (TODO)

The C<graft> command is optional, and is principally for those who
want to override the decisions that this tool makes (and modifies what
is shown ahead of time using the C<describe> sub-command, see below).

If you just specify two change numbers, then the detected branch paths
for those changes is used.  Bear in mind that this detection will
frequently be wrong, unless you already have a partial conversion
underway.

When grafts are specified, they I<replace> the detected parents
completely.  Use C<-a> to add to the detected/already configured
parents.

To get around this, specify your grafting with the path as well, eg:

   git-p4raw -a //depot/perl@18 //depot/ansiperl@17

This would say that the branch at C<//depot/ansiperl> change 17 should
be added to the parents of C<//depot/perl> change 18.

If a "change" parses as a git commit ID via C<git rev-parse>, it is
treated as external to the data being imported.  Do B<not> use commit
IDs or references from previous import runs here!

The C<-d> option will reset existing grafts for a change.

This command dumps all of the configured changes to the file
F<p4raw-extra-grafts.asv> on completion, so that it can be re-loaded
should a subsequent C<git-p4raw drop> happen.  The data in this table
is unable to be checked for integrity at C<git-p4raw check> time.

=item C<git-p4raw export [ CHG[..CHG] | -n NUM ]> (TODO)

=item C<git-p4raw export-blobs [ CHG[..CHG] | -n NUM ]>

=item C<git-p4raw export-commits [ CHG[..CHG] | -n NUM ]> (TODO)

Export a bunch of changes (or, just the blobs, or just the commits)
bounded by a range of revisions or the next NUM revisions, to a C<git
fast-import> process.  If this command is started with standard output
as a terminal, it will start C<git fast-import> itself.

If you start C<git fast-import>, you must take care to pass
C<--export-marks> to it if you want to perform subsequent import runs.
Also you will need to remove the foreign key constraints on rev_marks
and change_marks.

=head3 export common options

=over

=item C<-n=INT>

Size of a "chunk" for export.

=item C<--depth=N>

Specify the delta depth.  If you are working on projects with a long
history this can save a lot of space.  Good for archives of old history.

=item  

=back

=back

=head2 INFORMATIONAL COMMANDS

These other commands are also mostly implemented, to show information
from the database.  Many of these commands closely correspond to their
C<p4> counterparts, though some are unique to this tool.

=over

=item C<git-p4raw find-change N>

(post-conversion) Just show the mapped git commits for a change
number.  There may be more than one if the change affected multiple
branches.  This command will be modelled on C<git-svn find-rev>, so
also be capable of showing the Perforce change number from a git
commit ID.  Currently this only looks at information in the database.

=item C<git-p4raw describe N>

Show change N vital statistics (TODO: post-conversion, mapped git
commits), and the files that were modified in that change.  A bit like
C<p4 describe -s> or C<git show --stat>.

This command performs a fairly mammoth query to determine whether
changes with lots of "integrate" changes look enough like a
cross-merge to be represented as such.  This works by looking at the
revisions of the source paths for 'integrate' changes, and then
finding out if any of those paths have new revisions between the one
that was 'integrated' and the change number in question.  It then
lists the change numbers that the revisions being integrated came
from, or "HEAD" if that was the most recent revision on that branch at
that time.  Thus, if you see output like this:

 On path //depot/perl,
         delete from //depot/relperl (HEAD), 27 file(s)
         branch from //depot/relperl (HEAD), 291 file(s)
         integrate from //depot/relperl (HEAD), 392 file(s)

Then the integration information does not contradict the assertion
that the change in question is a cross-merge from F<//depot/relperl>
to F<//depot/perl>.

However, this is not yet quite enough to prove that the integration
information in the change is a cross-merge; we also need to show that
there are changes between the I<merge base> of the two branches that
could have been merged, but are not mentioned.  That half of the
equation is currently TODO.

=item C<git-p4raw integrated DEPOTPATH>

Mimic the 'p4 integrated' command precisely, from the database
information.

With git you'd probably use something like C<gitk --all -- filename>
to show all changes that touched a particular file and their relative
ancestry.

=item C<git-p4raw filelog DEPOTPATH>

Mimic the 'p4 filelog' command precisely, from the database
information.

Again, you'd probably just use the C<gitk> file selector or perhaps
even C<git log> with the same options to show this information.  The
principle difference being you won't see things like C<copy into>,
C<branch into>, etc - only the C<from> relationships.

=item C<git-p4raw changes [ -m NUM | -l ] [ file[revRange] ]>

Mimic the 'p4 changes' command closely from the database information.

C<-m NUM> is like C<git log -NUM>, and the C<-l> is like dropping the
C<--pretty=oneline> from the concise C<git-log --pretty=oneline>
command (see also L<git-config/alias.*>).  C<git-log> also supports
adding C<file>, but the C<revRange> thing is a perforce-ism not
carried over.

The C<-i> option is basically implied by C<git-log>, so long as the
"integrate" node was a branch cross-merge.  For cherry picking, the
equivalent command will be something like C<git-log --left-right
--cherry-pick otherbranch...HEAD -- filename>, and won't see cases
where the change was picked from another change with modifications.
However, there I<will> be a note in the changelog that the file was
picked when using C<git-log>, and as a last resort there is always
this command.

=item C<git-p4raw print [ -c CHANGE ] [ -Q ] [ -a ] file[rev]>

Prints the contents of a Perforce file at a particular revision.
C<-q> is a standard Scriptalicious switch, so suppress the header with
C<-Q>.

Currently this command does not vary its output like C<p4 print> does
based on the filetype, to add trailing linefeeds and the like, perform
CR/LF conversion or keyword expansion.

You can print all versions of the file with C<-a> as in Perforce, but
revision selectors are still TODO.

Specify a change number with C<-c NUM>.

=item C<git-p4raw ls-tree branch[@CHANGE]>

Show the contents of a particular Perforce path at particular
change.  Defaults to the latest change.

=item C<git-p4raw annotate file[revRange]> (TODO)

Run C<git annotate> on the final result for the equivalent of this.
However, this command will print the (head) revisions for the given
depotpath which C<git annotate> would not normally search; this
information could be used with C<git annotate --incremental> to
produce the same results as C<p4 annotate>.

That problem goes away in practice with a changeset-based development
style, so this is mostly for getting the best annotation possible for
the current Perforce changes.

=back

As this program is not currently envisioned to support committing back
to Perforce, these commands will never be implemented, but details on
how the same thing is achieved with git follows;

=over

=item C<p4 sync>

This is something like C<git pull>, which is a C<git fetch> and a
C<git merge> in one.  However, in git you should always commit (or
stash) local changes first.

C<p4 sync -f> is very much like C<git merge -s theirs>, similarly
C<p4 sync -k> is like C<git merge -s ours>

The C<-n> option, well with git you normally just stash your changes
somewhere safe, merge and see what happens.  If you don't like it you
can C<git reset> back to where you were.  But if you like, the
plumbing command C<git read-tree -m> with a temporary "index" (read
the man page) can be used to do exactly the same thing (say, in a
script).

=item C<p4 opened>

This is very similar to the git 1.5.3+ command C<git stash list>
followed by C<git stash show>.  Or the alternate porcelain system
called "stacked git".

=item C<p4 edit>

With C<git add> you mark a file (in its current state) as belonging to
the commit/change that you are working on.  Saving files as belonging
to a particular stashed change can be done by stashing it with a new
name, pulling out the old stash and cherry picking the new stash on
top.  This is certainly one area where stacked git (currently)
provides much better command set, if less integration with
hunk-picking tools like "git gui".

=item C<p4 diff>

C<-du> is the default to C<git diff>.  You'll have to translate the
arguments to diff using the C<describe> or C<find-change>

=item C<p4 submit>

The command is C<git push>.  Assuming you have access (or maybe you're
pushing to an open access/mob branch), you can push any upstream
branch forward that has its HEAD commit in your ancestry.  So,
atomicity is enforced in that manner - if someone else pushes first,
you need to merge their commit in before pushing yours.  However you
can always push to a new branch.

Creating a new branch is not an issue; the names are considered
temporary pointers which can safely be discarded if the revision is
merged, or even thrown into a non-default ref space such as
C<refs/Attic/...> (sick sense of humour, I know) so that they don't
clutter, should the change be considered not worth preserving, or
should it get fixed up into a better change.  It can even get deleted.

=item C<p4v>

Currently the major git UIs are C<git gui> (for making commits, or
amending the last commit) and C<gitk> (for viewing/searching the
commit graph, inspecting commits, and some limited amount of changes
like cherry picking).

=back

=cut

# Local Variables:
#   mode: cperl
#   cperl-brace-offset: 0
#   cperl-continued-brace-offset: 0
#   cperl-indent-level: 8
#   cperl-label-offset: -8
#   cperl-merge-trailing-else: nil
#   cperl-continued-statement-offset: 8
# End:
